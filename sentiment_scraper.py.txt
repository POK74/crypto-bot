import logging
import aiohttp
import os
from urllib.parse import quote
from textblob import TextBlob
from datetime import datetime, timedelta
import json
import asyncio

logger = logging.getLogger(__name__)

CACHE_FILE = "sentiment_cache.json"
CACHE_TTL = timedelta(minutes=30)

# Hent fra cache hvis tilgjengelig og fersk
async def load_from_cache(coin: str) -> float:
    try:
        if not os.path.exists(CACHE_FILE):
            return None
        with open(CACHE_FILE, "r") as f:
            cache = json.load(f)
        if coin in cache:
            ts = datetime.fromisoformat(cache[coin]["timestamp"])
            if datetime.utcnow() - ts < CACHE_TTL:
                return cache[coin]["score"]
    except Exception as e:
        logger.warning(f"Cache error: {e}")
    return None

# Lagre score i cache
async def save_to_cache(coin: str, score: float):
    try:
        cache = {}
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r") as f:
                cache = json.load(f)
        cache[coin] = {"score": score, "timestamp": datetime.utcnow().isoformat()}
        with open(CACHE_FILE, "w") as f:
            json.dump(cache, f)
    except Exception as e:
        logger.warning(f"Could not write to cache: {e}")

# Full sentimentanalyse med Google Trends, Reddit, Twitter og fallback-cache
async def get_sentiment_score(coin: str) -> float:
    cached = await load_from_cache(coin)
    if cached is not None:
        logger.info(f"Loaded sentiment score for {coin} from cache: {cached}")
        return cached

    sentiment = 0.0

    try:
        async with aiohttp.ClientSession() as session:
            # --- Google Trends ---
            trends_url = f"https://trends.google.com/trends/api/autocomplete/{quote(coin)}%20crypto"
            async with session.get(trends_url) as resp:
                if resp.status == 200:
                    data = await resp.text()
                    if "trending" in data.lower():
                        sentiment += 2.0

            # --- Reddit mentions + NLP ---
            reddit_url = f"https://api.pushshift.io/reddit/search/submission/?q={quote(coin)}&subreddit=CryptoCurrency&size=20"
            async with session.get(reddit_url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    posts = data.get("data", [])
                    mentions = len(posts)
                    total_polarity = 0.0
                    for post in posts:
                        title = post.get("title", "")
                        blob = TextBlob(title)
                        total_polarity += blob.sentiment.polarity
                    avg_sentiment = total_polarity / mentions if mentions else 0
                    if mentions > 15:
                        sentiment += 1.0
                    if avg_sentiment > 0.1:
                        sentiment += 2.0
                    elif avg_sentiment < -0.1:
                        sentiment -= 1.0

            # --- Twitter / X via SerpAPI ---
            serpapi_key = os.getenv("SERPAPI_KEY", "demo")
            serp_url = f"https://serpapi.com/search.json?engine=twitter&q={quote(coin)}+crypto&api_key={serpapi_key}"
            async with session.get(serp_url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    tweets = data.get("tweets", [])
                    if len(tweets) > 5:
                        sentiment += 1.5

            # --- Google News (via SerpAPI) ---
            news_url = f"https://serpapi.com/search.json?q={quote(coin)}+crypto&engine=google_news&api_key={serpapi_key}"
            async with session.get(news_url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    news_results = data.get("news_results", [])
                    for article in news_results[:5]:
                        title = article.get("title", "")
                        blob = TextBlob(title)
                        sentiment += blob.sentiment.polarity * 1.5

        final_score = round(max(0.0, min(sentiment, 10.0)), 2)
        await save_to_cache(coin, final_score)
        return final_score

    except Exception as e:
        logger.warning(f"Sentiment analysis error for {coin}: {e}")
        return 0.0
